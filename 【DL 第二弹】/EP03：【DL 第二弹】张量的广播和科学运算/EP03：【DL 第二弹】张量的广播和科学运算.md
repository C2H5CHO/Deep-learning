# 一、数学运算
## 1.1 算子
在深度学习中，张量（Tensor）作为承载数据和参与计算的核心载体，其运算能力直接决定了模型的构建与训练效率。PyTorch 作为主流的深度学习框架，将所有可作用于张量的运算统一称为**算子**，这一概念的提出并非偶然 —— 它本质上是对张量运算的规范化抽象。

不同于 NumPy 等传统数值计算库对运算的松散分类，PyTorch 的算子体系更贴合深度学习的工程需求。算子可以理解为**操作单元**，无论是简单的加法、乘法，还是复杂的矩阵分解、傅里叶变换，都可被封装为算子。这种设计的优势在于：一方面，用户能通过统一的接口调用不同运算，降低学习成本；另一方面，框架可对算子进行针对性优化（如 GPU 加速、自动微分支持），提升计算性能。

在实际应用中，算子是连接数据与模型的桥梁。例如，神经网络中的卷积层本质上是通过卷积算子对输入张量进行特征提取，激活函数则是逐点算子对张量元素的非线性变换。理解算子的概念，是掌握 PyTorch 张量运算逻辑的基础 —— 它不仅是**函数**的另一种称呼，更代表了框架对运算流程的标准化管理。
## 1.2 分类
为了让用户更高效地调用运算，PyTorch 将张量的数学运算划分为六大类，每类都有明确的适用场景，这种分类方式既符合数学逻辑，也贴合深度学习的实际需求。

- 逐点运算（Pointwise Ops）：这类运算的核心是**逐个元素处理**，即对张量中的每个元素执行相同的操作，且元素间的计算相互独立。由于运算过程可并行化，逐点运算在 GPU 上能高效执行，是神经网络中激活函数、数据预处理等步骤的基础。
- 规约运算（Reduction Ops）：与逐点运算不同，规约运算的目标是从张量中**提炼总结信息**，最终输出一个或少数几个值。在模型训练中，损失函数的计算（如 MSE 损失）就依赖规约运算，将批量数据的误差聚合为一个标量用于反向传播。
- 比较运算（Comparison Ops）：主要用于判断张量间的关系，输出布尔类型的张量。这类运算常用于逻辑判断，比如在数据筛选中保留满足条件的元素，或在模型推理中判断预测结果是否符合阈值。
- 谱运算（Spectral Ops）：聚焦于信号处理领域的傅里叶变换相关操作，包括傅里叶变换、逆傅里叶变换等。在深度学习中，谱运算可用于处理图像、音频等信号数据，例如通过傅里叶变换将图像从空间域转换到频率域，提取高频或低频特征。
- BLAS 和 LAPACK 运算：这两类运算源自经典的线性代数程序库，包含了矩阵乘法、矩阵分解（如 LU 分解、Cholesky 分解）、线性方程组求解等核心线性代数操作。在深度学习中，全连接层的计算本质是矩阵乘法，卷积层也可通过矩阵运算实现，因此 BLAS 和 LAPACK 运算堪称框架的**算力基石**。
- 其他运算（Other Ops）：涵盖了无法被上述类别包含的特殊运算，例如张量的形状调整（`reshape`）、维度扩展（`unsqueeze`）等。这类运算虽不直接参与数值计算，但在数据预处理、张量形状适配（如满足广播条件）中发挥着关键作用。
# 二、张量的广播特性
## 2.1 相同形状张量的计算
广播特性是 PyTorch 中张量运算的核心机制之一，其本质是==在不复制数据的前提下，让不同形状的张量兼容运算==。很多人误以为广播只发生在不同形状的张量之间，实则不然 —— 相同形状的张量运算，本质上也是广播的一种特殊情况。

官网明确指出 “same shapes are always broadcastable”（相同形状的张量始终可广播），这是因为当两个张量形状完全一致时，它们的每个维度大小都相等，天然满足广播的**维度兼容**条件。此时，运算会在对应位置的元素间进行，例如两个形状为 (3,) 的张量相加，就是第一个元素加第一个元素、第二个加第二个，以此类推。

这种**相同形状的广播**看似简单，却是更复杂广播逻辑的基础。它背后的设计逻辑是：无论张量形状是否相同，运算都遵循**元素级对应计算**的原则，而广播机制则负责解决**如何让不同形状的张量实现对应计算**的问题。例如，在神经网络的批量训练中，输入数据的批量维度、特征维度通常保持一致，此时的矩阵乘法、激活函数等操作，都是基于相同形状的广播机制实现的高效元素级计算。

```python
t1 = torch.arange(3)
print(f"t1：{t1}")
print(f"t1+t1：{t1+t1}")
```
- 运行结果：
```
t1：tensor([0, 1, 2])
t1+t1：tensor([0, 2, 4])
```
## 2.2 不同形状张量的计算
当两个张量形状不同时，广播机制通过**隐式扩展**让它们兼容运算，但这种扩展并非无规则 —— 只有满足特定条件的张量才能广播。理解这些规则，是避免运算错误（如**维度不匹配**）的关键。
### 2.2.1 标量 + 任意形状的张量
标量（0 维张量）可以与任意形状的张量直接运算，这是广播机制中最直观的场景。此时，标量会被**隐式扩展**为与另一个张量形状完全相同的张量，扩展后的每个元素都等于原标量，再与原张量进行元素级运算。

在实际应用中，这种运算非常常见：比如给图像的每个像素值加一个偏移量（亮度调整）、对模型输出的每个元素进行归一化（减去均值）等，都依赖标量与张量的广播特性，既简洁又高效。

```python
t2 = torch.arange(3)
print(f"t2+1：{t2+1}")
```
- 运行结果：
```
t2+1：tensor([1, 2, 3])
```
- 示例解读：
> 形状为 (3,) 的张量`t2 = [0,1,2]`与标量1相加时，标量1会被扩展为`[1,1,1]`，最终结果为`[1,2,3]`。这种机制极大简化了代码 —— 无需手动将标量转换为同形状张量，框架会自动完成扩展，且不会额外占用内存（扩展是逻辑上的，而非实际复制数据）。
### 2.2.2 相同维度、不同形状张量的计算
当两个张量维度相同但形状不同时，广播的核心规则是：对于每个维度，两个张量的大小要么相等，要么其中一个为 1。若某维度大小不满足这一条件，则无法广播，会抛出**维度不匹配**的错误。

1. (n, m) + (1, m)/(n, 1)
```python
t3 = torch.zeros(3, 4)
print(f"t3：{t3}")
t4 = torch.ones(1, 4)
print(f"t4：{t4}")
print(f"t3+t4：{t3+t4}")

# ×1. 若二者取值均不为1，则无法广播
t5 = torch.ones(2, 4)
# print(f"t3+t5：{t3+t5}") # RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0
```
- 运行结果：
```
t3：tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])
t4：tensor([[1., 1., 1., 1.]])
t3+t4：tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
```
- 示例解读：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/78a8ce08de0545fb8e65fa1afa8eb9dc.png#pic_center)
> 形状为 (3,4) 的`t3`与 (1,4) 的`t4`相加时，`t4`的第一个维度大小为 1，会被扩展为 3（与`t3`的第一个维度匹配），扩展后形状为 (3,4)，最终实现对应元素相加。但若将`t4`换成形状为 (2,4) 的`t5`，由于第一个维度（3 vs 2）既不相等也不为 1，无法广播，运算会报错。

2. (1, m) + (n, 1)

```python
t6 = torch.arange(3).reshape(1, 3)
t7 = torch.arange(3).reshape(3, 1)
print(f"t6：{t6}，\nt7；{t7}，\nt6+t7：{t6+t7}")
```
- 运行结果：
```
t6：tensor([[0, 1, 2]])，
t7；tensor([[0],
        [1],
        [2]])，
t6+t7：tensor([[0, 1, 2],
        [1, 2, 3],
        [2, 3, 4]])
```
- 示例解读：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/d4ca3ced70df44969491fdd6659eb1c6.png#pic_center)
> 形状为 (1,3) 的`t6`与 (3,1) 的`t7`相加，`t6`会扩展为 (3,3)（每行都是`[0,1,2]`），`t7`会扩展为 (3,3)（每列都是`[0,1,2]`），最终结果为两个扩展张量的对应元素相加，形成类似**加法表**的矩阵。这种场景在生成网格数据（如坐标点 (x+y) 计算）中非常实用。

3. 三维张量

```python
t8 = torch.zeros(3, 4, 5)
t9 = torch.arange(3, 4, 1)
print(f"t8：{t8}，\nt9：{t9}，\nt8+t9：{t8+t9}")

t10 = torch.ones(1, 1, 5)
print(f"t8+t10：{t8+t10}")
```
- 运行结果：
```
t8：tensor([[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])，
t9：tensor([3])，
t8+t9：tensor([[[3., 3., 3., 3., 3.],
         [3., 3., 3., 3., 3.],
         [3., 3., 3., 3., 3.],
         [3., 3., 3., 3., 3.]],

        [[3., 3., 3., 3., 3.],
         [3., 3., 3., 3., 3.],
         [3., 3., 3., 3., 3.],
         [3., 3., 3., 3., 3.]],

        [[3., 3., 3., 3., 3.],
         [3., 3., 3., 3., 3.],
         [3., 3., 3., 3., 3.],
         [3., 3., 3., 3., 3.]]])
t8+t10：tensor([[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]])
```
- 示例解读：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/d054670db3c94f7b8e625ce27a429363.png#pic_center)
> 形状为 (3,4,5) 的`t8`与 (1,1,5) 的`t10`相加时，`t10`的前两个维度（1,1）会分别扩展为 3 和 4，与`t8`形状匹配后再运算。这体现了广播的**维度对齐**逻辑 —— 从最后一个维度开始向前检查，每个维度都需满足**相等或为 1**的条件。

### 2.2.3 不同维度张量的计算
当两个张量维度不同时，广播的前提是==低维张量可通过升维适配高维张量==。升维的规则是：在低维张量的前面添加大小为 1 的维度，直到两者维度相同，再按照**相同维度不同形状**的规则进行广播。

这种机制解决了高维与低维数据的兼容问题。例如，在处理视频数据（4 维：批量 × 时间 × 高度 × 宽度）时，若要给每个时间帧的每个像素加一个相同的 2 维特征（高度 × 宽度），就可通过升维广播实现，无需手动重复特征数据。

```python
t11 = torch.arange(4).reshape(2, 2)
t12 = torch.zeros(3, 2, 2)
# 转为3维张量
t11.reshape(1, 2, 2)
print(f"t11+t12：{t11+t12}")
```
- 运行结果：
```
t11+t12：tensor([[[0., 1.],
         [2., 3.]],

        [[0., 1.],
         [2., 3.]],

        [[0., 1.],
         [2., 3.]]])
```
# 二、逐点运算
&emsp;&emsp;逐点运算（Pointwise Ops）是 PyTorch 中最基础、最常用的运算类型，其核心特点是==对张量中的每个元素执行独立的相同操作==，运算结果的形状与输入张量完全一致。这种运算的优势在于可高度并行化，非常适合 GPU 加速，因此在深度学习的各个环节（如数据预处理、激活函数计算、损失函数中间过程等）被广泛使用。
## 2.1 Tensor数学基本运算
张量的基本数学运算包括加、减、乘、除，它们既可以通过函数（如`torch.add`）调用，也可以通过运算符（如`+`）直接实现，两者效果完全一致。

这些运算的前提是两个张量可广播（形状兼容）。在实际应用中，它们是构建复杂运算的基础，例如神经网络中 “输入 × 权重 + 偏置” 的计算，就结合了乘法和加法的逐点运算。

|**函数**|**描述**|
| :------:| :------: |
| `torch.add(t1，t2)`      | `t1`、`t2`两个张量逐个元素相加，等效于`t1+t2`     |
| `torch.subtract(t1，t2)` | `t1`、`t2`两个张量逐个元素相减，等效于`t1-t2`      |
| `torch.multiply(t1，t2)` | `t1`、`t2`两个张量逐个元素相乘，等效于`t1*t2`            |
| `torch.divide(t1，t2)`   | `t1`、`t2`两个张量逐个元素相除，等效于`t1\t2`            |

```python
t1 = torch.tensor([1, 2, 3])
t2 = torch.tensor([4, 5, 6])
print(f"相加：{torch.add(t1, t2)}")
print(f"相减：{torch.subtract(t2, t1)}")
print(f"相乘：{torch.multiply(t1, t2)}")
print(f"相除：{torch.divide(t2, t1)}")
```
- 运行结果：
```
相加：tensor([5, 7, 9])
相减：tensor([3, 3, 3])
相乘：tensor([ 4, 10, 18])
相除：tensor([4.0000, 2.5000, 2.0000])
```
## 2.2 Tensor数值调整函数
数值调整函数用于对张量元素的数值进行转换，常见的包括取绝对值、取整、求相反数等，它们均为逐点操作，且部分函数提供**原地修改**版本（函数名后加下划线，如`neg_`）。

原地修改版本（如`neg_`）会直接修改输入张量的值，而不创建新张量，可节省内存，尤其在处理大型张量时优势明显。

|**函数**|**描述**|
| :------:| :------: |
| `torch.abs(t)`        | 返回绝对值 | 
| `torch.ceil(t)`       | 向上取整 | 
| `torch.floor(t)`      | 向下取整 |
| `torch.round(t)`      | 四舍五入取整 |
| `torch.neg(t)`      | 返回相反的数 |

```python
t2 = torch.randn(5)
print(f"绝对值：{torch.abs(t2)}")
print(f"向上取整：{torch.ceil(t2)}")
print(f"向下取整：{torch.floor(t2)}")
print(f"四舍五入取整：{torch.round(t2)}")
print(f"相反数：{torch.neg(t2)}")

# *1. 若需要对原对象本身进行修改，则考虑使用 方法_()
print(f"t2：{t2}")
t2_ = torch.neg_(t2)
print(f"t2_：{t2_}")
print(f"t2：{t2}")
# *2. 许多科学计算也都有同名方法
t2_e = torch.exp_(t2)
print(f"t2_e：{t2_e}")
print(f"t2：{t2}")
```
- 运行结果：
```
绝对值：tensor([0.6867, 0.2683, 0.9146, 0.1978, 0.6326])
向上取整：tensor([1., 1., 1., 1., -0.])
向下取整：tensor([ 0.,  0.,  0.,  0., -1.])
四舍五入取整：tensor([ 1.,  0.,  1.,  0., -1.])
相反数：tensor([-0.6867, -0.2683, -0.9146, -0.1978,  0.6326])
t2：tensor([ 0.6867,  0.2683,  0.9146,  0.1978, -0.6326])
t2_：tensor([-0.6867, -0.2683, -0.9146, -0.1978,  0.6326])
t2：tensor([-0.6867, -0.2683, -0.9146, -0.1978,  0.6326])
t2_e：tensor([0.5032, 0.7647, 0.4007, 0.8206, 1.8824])
t2：tensor([0.5032, 0.7647, 0.4007, 0.8206, 1.8824])
```
## 2.3 Tensor数据科学运算
这类运算涵盖了幂运算、对数运算、三角函数等科学计算中常用的操作，均为逐点运算，且与数学定义完全一致。

需要注意的是，这些函数仅作用于张量对象（不能直接传入整数），且对于定义域有要求（如`sqrt`输入需非负，否则返回`nan`）。实际使用中，幂运算与对数运算常结合使用（如`exp(log(t))`可近似还原 `t`，用于验证计算正确性）。

|**数学运算函数**|**数学公式**|**描述**|
| :------:| :------: | :------: |
| 幂运算 |
| `torch.exp(t)`        | $y_{i} = e^{x_{i}}$ | 返回以e为底、`t`中元素为幂的张量 | 
| `torch.expm1(t)`         | $y_{i} = e^{x_{i}} - 1$ |对张量中的所有元素计算`exp(x)-1` |
| `torch.exp2(t)`          | $y_{i} = 2^{x_{i}}$ |逐个元素计算2的`t`次方 | 
| `torch.pow(t, n)`       | $\text{out}_i = x_i ^ \text{exponent}$ | 返回`t`的`n`次幂 | 
| `torch.sqrt(t)`       |$\text{out}_{i} = \sqrt{\text{input}_{i}}$ | 返回`t`的平方根 | 
| `torch.square(t)`        |$\text{out}_i = x_i ^ \text{2}$ | 返回输入的元素平方                     | 
| 对数运算 |
| `torch.log10(t)`      |$y_{i} = \log_{10} (x_{i})$ | 返回以10为底的`t`的对数 | 
| `torch.log(t)`  |$y_{i} = \log_{e} (x_{i})$| 返回以e为底的`t`的对数 |
| `torch.log2(t)`          |$y_{i} = \log_{2} (x_{i})$| 返回以2为底的`t`的对数                         | 
| `torch.log1p(t)`         |$y_i = \log_{e} (x_i + 1)$| 返回一个加自然对数的输入数组     | 
| 三角函数运算|
| `torch.sin(t)`           |三角正弦                               | 
| `torch.cos(t)`           | 元素余弦                               | 
| `torch.tan(t)`           |逐元素计算切线                         | 

```python
# ×1. 只能作用于tensor对象
# print(f"2的2次方 整型：{torch.pow(2, 2)}") # TypeError: pow() received an invalid combination of arguments - got (int, int), but expected one of
print(f"2的2次方 0维张量：{torch.pow(torch.tensor(2), 2)}")
# *2. 具有一定的静态性
t3 = torch.arange(1, 4)
print(torch.expm1(t3))
# *3. 区分2的t次方和t的2次方
t4 = torch.randn(5)
print(torch.square(t4))
print(torch.sqrt(t4))
print(torch.pow(t4, 2))
print(torch.pow(t4, 0.5))
# *4. 幂运算 VS 对数运算
t5 = torch.tensor([1, 2, 3])
print(torch.exp(torch.log(t5)))
print(torch.exp2(torch.log2(t5)))
```
- 运行结果：
```
2的2次方 0维张量：4
tensor([ 1.7183,  6.3891, 19.0855])
tensor([2.9722e-01, 1.4908e+00, 3.3102e+00, 1.1289e-03, 3.2466e-01])
tensor([0.7384, 1.1050, 1.3488,    nan,    nan])
tensor([2.9722e-01, 1.4908e+00, 3.3102e+00, 1.1289e-03, 3.2466e-01])
tensor([0.7384, 1.1050, 1.3488,    nan,    nan])
tensor([1., 2., 3.])
tensor([1., 2., 3.])
```
## 2.4 `sort()` 排序
排序在很多场景中不可或缺：例如在 `top-k` 准确率计算中，通过排序找到模型预测概率最高的 `k` 个类别；在数据预处理中，对特征值排序后进行归一化等。索引`indices`的作用在于追溯原数据位置，这在需要保留原始信息时非常重要。

```python
t6 = torch.tensor([1, 3, 5, 2, 4])
print(f"升序：{torch.sort(t6)}")
print(f"降序：{torch.sort(t6, descending=True)}")
```
- 运行结果：
```
升序：torch.return_types.sort(
values=tensor([1, 2, 3, 4, 5]),
indices=tensor([0, 3, 1, 4, 2]))
降序：torch.return_types.sort(
values=tensor([5, 4, 3, 2, 1]),
indices=tensor([2, 4, 1, 3, 0]))
```
# 三、规约运算
&emsp;&emsp;规约运算（Reduction Ops）是一类**聚合型**运算，它通过对张量元素进行汇总计算，将一个或多个维度的信息压缩为少数值（甚至标量）。这类运算在数据分析、模型评估、损失计算等场景中至关重要，因为它能从海量数据中提取关键统计特征。

## 3.1 Tensor统计分析函数
统计分析函数是规约运算的核心，它们通过不同的聚合逻辑从张量中提取统计信息。这些函数的核心是**信息压缩**，例如对一个形状为 (1000, 100) 的特征张量求均值，可得到一个标量，快速判断数据整体分布趋势。

|**函数**|**描述**|
| :------:| :------: |
| `torch.mean(t)`        | 返回张量均值 | 
| `torch.var(t)`       | 返回张量方差 | 
| `torch.std(t)`        | 返回张量标准差 | 
| `torch.var_mean(t)`       | 返回张量方差和均值 | 
| `torch.std_mean(t)`       | 返回张量标准差和均值 | 
| `torch.max(t)`        | 返回张量最大值 | 
| `torch.argmax(t)`        | 返回张量最大值索引 | 
| `torch.min(t)`       | 返回张量最小值 | 
| `torch.argmin(t)`       | 返回张量最小值索引 | 
| `torch.median(t)`        | 返回张量中位数 | 
| `torch.sum(t)`       | 返回张量求和结果 | 
| `torch.logsumexp(t)`       | 返回张量各元素求和结果，适用于数据量较小的情况 | 
| `torch.prod(t)`        | 返回张量累乘结果 | 
| `torch.dist(t1, t2)`        | 计算两个张量的闵式距离，可使用不同范式 |
| `torch.topk(t)`        | 返回t中最大的`k`个值对应的指标 |
## 3.2 `dist()` 闵可夫斯基距离
`torch.dist(t1, t2, p)`用于计算两个张量（需形状相同）的闵可夫斯基距离，其数学公式为：
$$
D(x,y) = (\sum^{n}_{u=1}|x_u-y_u|^{p})^{1/p}
$$
其中`p`为距离参数，不同的`p`对应不同的距离类型：
- 当`p=1`时，为曼哈顿距离（街道距离），计算各元素差值的绝对值之和，适用于网格状空间（如城市道路距离）。
- 当`p=2`时，为欧式距离，计算差值平方和的平方根，是最常用的距离度量（如样本相似度计算）。
- 当`p→∞`时，为切比雪夫距离，等于元素差值的最大值，适用于需关注最大差异的场景。

```python
t1 = torch.tensor([1.0, 2.0, 3.0])
t2 = torch.tensor([4.0, 5.0, 6.0])
print("曼哈顿距离：", torch.dist(t1, t2, 2))
print("街道距离：", torch.dist(t1, t2, 1))
```
- 运行结果：
```
曼哈顿距离： tensor(5.1962)
街道距离： tensor(9.)
```
## 3.3 规约运算的维度
对于高维张量，规约运算可通过`dim`参数指定**沿哪个维度进行聚合**，这是理解高维数据处理的关键。指定`dim`后，运算会沿该维度对元素进行聚合，结果会**消除**该维度（或保留大小为 1 的维度，取决于`keepdim`参数）。

在实际应用中，维度的选择取决于分析目标：例如对批量图像（形状为 `[批量数，高度，宽度]`）求均值，`dim=0`得到所有图像的平均像素，`dim=(1,2)`得到每张图像的平均亮度。

```python
t2 = torch.arange(24).float().reshape(2 ,3 ,4)
print(t2)
print(torch.sum(t2, dim=0))
print(torch.sum(t2, dim=1))
```
- 运行结果：
```
tensor([[[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.]],

        [[12., 13., 14., 15.],
         [16., 17., 18., 19.],
         [20., 21., 22., 23.]]])
tensor([[12., 14., 16., 18.],
        [20., 22., 24., 26.],
        [28., 30., 32., 34.]])
tensor([[12., 15., 18., 21.],
        [48., 51., 54., 57.]])
```
## 3.4 2维张量的排序
二维张量的排序与一维类似，但可通过`dim`参数指定按行（`dim=1`）或按列（`dim=0`）排序，同时支持`descending`参数控制升降序。

这种排序在表格数据处理中非常实用，例如按分数列（`dim=0`）对学生成绩表排序，或按各科成绩行（`dim=1`）对单个学生的成绩排序。

```python
t3 = torch.randn(3 ,4)
print(t3)
print(f"默认按行进行升序：{torch.sort(t3)}")
print(f"按列进行降序：{torch.sort(t3, dim=1, descending=True)}")
```
- 运行结果：
```
tensor([[-0.9374, -0.6428,  0.5662, -0.0928],
        [ 0.6922, -0.7024, -0.4042,  0.3694],
        [ 0.4550, -1.1046,  0.7593,  1.5569]])
默认按行进行升序：torch.return_types.sort(
values=tensor([[-0.9374, -0.6428, -0.0928,  0.5662],
        [-0.7024, -0.4042,  0.3694,  0.6922],
        [-1.1046,  0.4550,  0.7593,  1.5569]]),
indices=tensor([[0, 1, 3, 2],
        [1, 2, 3, 0],
        [1, 0, 2, 3]]))
按列进行降序：torch.return_types.sort(
values=tensor([[ 0.5662, -0.0928, -0.6428, -0.9374],
        [ 0.6922,  0.3694, -0.4042, -0.7024],
        [ 1.5569,  0.7593,  0.4550, -1.1046]]),
indices=tensor([[2, 3, 1, 0],
        [0, 3, 2, 1],
        [3, 2, 0, 1]]))
```
# 四、比较运算
&emsp;&emsp;比较运算用于判断张量间的元素关系或整体一致性，返回布尔类型的张量（元素为`True`或`False`），是逻辑判断和数据筛选的基础工具。

&emsp;&emsp;这些运算在实际中应用广泛：例如在目标检测中，用`gt`筛选出置信度大于阈值的预测框；在数据清洗中，用`eq`找到缺失值（如与`nan`比较）；在模型验证中，用`equal`检查两次训练结果是否一致（排除随机因素干扰）。

- Tensor比较运算函数

|**函数**|**描述**|
| :------:| :------: |
| `torch.eq(t1, t2)`        | 比较`t1`、`t2`各元素是否相等，等效`==`| 
| `torch.equal(t1, t2)`       | 判断两个张量是否是相同的张量 | 
| `torch.gt(t1, t2)`        | 比较`t1`各元素是否大于`t2`各元素，等效`>`| 
| `torch.lt(t1, t2)`        | 比较`t1`各元素是否小于`t2`各元素，等效`<`| 
| `torch.ge(t1, t2)`        | 比较`t1`各元素是否大于或等于`t2`各元素，等效`>=`| 
| `torch.le(t1, t2)`        | 比较`t1`各元素是否小于等于`t2`各元素，等效`<=`| 
| `torch.ne(t1, t2)`        | 比较`t1`、`t2`各元素是否不相同，等效`!=`| 

```python
import torch

t1 = torch.tensor([1.0, 2, 3])
t2 = torch.tensor([1.0, 4, 5])
print(f"张量是否相同：{torch.equal(t1, t2)}")
print(f"元素是否相同：{torch.eq(t1, t2)}")
```
- 运行结果：
```
张量是否相同：False
元素是否相同：tensor([ True, False, False])
```
- 示例解读：
> 需要注意的是，`eq`与`equal`的区别：`eq`关注元素级是否相等，`equal`关注张量整体是否完全一致。例如形状相同但元素部分不同的张量，`eq`返回包含`False`的张量，`equal`直接返回`False`。

----
==微语录：一颗尘埃的重量，藉藉无名也有光。——《浪浪山的小妖怪》==
